[
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Contact Information\n\nName: Pete Pritchard\nEmail: petep at umich dot edu\nPhone: (423) eight hundred-1887\n\n\nDownload Resume\n\n\nExperience\n\nSoftware Engineering Intern, NASA\nWashington, DC | June 2024 – August 2024\n\nWorked with a small, diverse team of interns with expertise in software engineering, UI/UX, strategic communication, and data science/engineering to enhance reserach and testing capabilties of national wind tunnel facilities\nUsed Python to build real-time data applications of operational and test data to provide comprehensive visualizations, reveal patterns and drive more effective decision-making and business outcomes\nEngineered an advanced analytics platform using Python, Google Cloud Storage, and Firestore, deployed via Docker on an existing NASA cloud environment\n\n\n\nData Science Intern, NASA\nWashington, DC | June 2023 – August 2023\n\nDeveloped short-term and long-horizon demand forecast models and deployed it along with a dashboard via Python and Plotly Dash\nDeveloped an API wrapper for a 20+ year old SAP database to allow financial managers quickly access budget reports and financial data\nAdded 30+ SQL queries to codebase and helped design demand database schema\nAssisted with the creation of performance and value scoring system and score cards for wind tunnel facilities\n\n\n\nGraduate Student Instructor, University of Michigan\nAnn Arbor, MI | August 2023 – Present\n\nLeading labs, grading assignments, and providing aid to 60+ students - Introduction to Stats and Data Analysis\nManaging canvas page, scheduling meetings and serving as primary communicator between students and professors\nReceived high student satisfaction reports and promoted to Graduate Student Mentor after one semester\n\n\n\nResearch Assistant, University of Tennessee\nChattanooga, TN | Junuary 2023 – May 2023\n\nSurveyed statistical and ML methods to predict the spread of viruses using Google Trends and Covid-19 data\nDeveloped an LSTM deep learning networks model with the use of R (Keras, Tensorflow)\nAchieved higher model performances compared with classic time series approaches (VAR, ARIMA)\nPresented findings at UT, Chattanooga’s research dialogue conference\n\n\n\nResearch Assistant, University of Tennessee\nChattanooga, TN | August 2022 – January 2023\n\nAssisted a project that formulated a new mixed-integer linear model to optimally locate Emergency Medical Service vehicle facilities to decrease response time, and ultimately save lives\nMy contributions included data assembly, running and analyzing tests on GPU clusters, and writing the report\nThe solution resulted in a 12.38 – 22.02 percent increase in overall coverage\n\n\n\nData Analytics Intern, Stray Dog Designs\nChattanooga, TN | May 2020 – August 2020\n\nAnalyzed campaign performance metrics to enhance social media marketing efforts\nCreated custom dashboards and automated weekly reports with Google Analytics and Data Studio\nDeveloped marketing mix models and clustering algorithms for customer segmentation to boost ROA\n\n\n\n\n\nEducation\n\nMS, Applied Statistics\nUniversity of Michigan, Ann Arbor | Month Year – Month Year\n\nCoursework: Regression, Probability Theory, Statistical Inference, Machine Learning, Time Series Analysis, Data Structures and Algorithms, Bayesian Modeling, Natural Language Processing, Design of Experiments\n\n\n\nBS, Applied Mathematics & Economics\nUniversity of Tennessee, Chattanooga | August 2020 – May 2023\n\n\n\n\nSkills\n\nLanguages: Python, SQL, R, C++, Java, HTML/CSS\nDeveloper Tools: Git, Google Cloud Platform, Airflow, DBT, Terraform, Linux\nLibraries: Pandas, NumPy, PySpark, PyTorch, Tensorflow, Keras, Scikit-learn, Plotly\nModeling: Supervised Learning, Neural Networks, Clustering, Time Series, Hidden Markov Process, Monte Carlo Methods\nSoft Skills: Spanish (Working level), Communication, Leadership\n\n\n\n\nAwards and Honors\n\nJohn C. Stophel Distinguished Student Award\nUniversity of Tennesee, Chattanooga | December 2022\n\nThe John C. Stophel Distinguished Student Award is presented annually to recognize high potential business students who have made significant and meaningful contributions to UTC, the Gary W. Rollins College of Business and their communities while maintaining a strong academic record.The John C. Stophel Distinguished Student Award is presented annually to recognize high potential business students who have made significant and meaningful contributions to UTC, the Gary W. Rollins College of Business and their communities while maintaining a strong academic record.\n\n\n\nZiad Keilany Scholarship in Economics\nUniversity of Tennesee, Chattanooga | December 2022\n\n\nJoe & Rachael Decosimo Scholarship\nUniversity of Tennesee, Chattanooga | December 2022\n\n\n\n\nReferences\nAvailable upon request."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Marcy Bot\n\n\nA discord fantasy football bot leveraging modern LLMs\n\n\n\n\n\n\nJun 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlternative Prey Hypothesis\n\n\nModeling the partially observed Markov process frameowork using a variation of the Lotka Voltera equations\n\n\n\n\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVOC Analysis\n\n\nInvestigation of the presence of volatile organic compounds (VOCs) in human blood samples and their relationship with various levels of exposure to tobacco products\n\n\n\n\n\n\nApr 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolar Flare Analysis\n\n\nIn this study, various classical time series methods are used in attempt to capture the implicit behavior in sunspot activity\n\n\n\n\n\n\nMar 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBSM Monte Carlo\n\n\nAn overview of the Black-Scholes European option pricing model and a solution using the Monte-Carlo stochastic modeling technique\n\n\n\n\n\n\nMay 15, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/marcy/index.html",
    "href": "projects/marcy/index.html",
    "title": "Marcy Bot",
    "section": "",
    "text": "In progress!!!"
  },
  {
    "objectID": "projects/pomp/index.html",
    "href": "projects/pomp/index.html",
    "title": "Alternative Prey Hypothesis",
    "section": "",
    "text": "Full report can be viewed here!\n\n\n\n\nMarkov Process\n\n\n\nAbstract\nStudying population dynamics is essential for understanding the complex interactions between species and their environment. By modeling population dynamics, researchers can gain an understanding of how animals respond to environmental changes and activities. This understanding is vital for conservation efforts as it aids in identifying endangered species and devising strategies to safeguard biodiversity. Furthermore, animal populations can act as reservoirs for infectious diseases, impacting both human and animal health. By comprehensively understanding population movements, scientists can predict outbreaks, discern transmission patterns, and enhance disease control and prevention measures.\nThis report extends research by Hjeljord and Loe, who explain the dwindling numbers of willow ptarmigan Lapagos lapagos in Northeastern Scandinavia. Although not in immediate danger, the willow ptarmigan faces threats from habitat loss, climate change, and hunting and trapping. Hjeljord and Loe postulate that, along with climate, a long-term dampening of the amplitude in small rodents and an increase in red fox numbers, have prevented willow ptarmigan populations from reaching their peaks seen a hundred years ago. Their analysis implements linear models with a count proxy as a function of time to estimate linear change and wavelet analysis to detect cyclic periods. This report’s aim is to further Hjelford and Loe’s work by capturing the stochastic population dynamics and the role of alternative prey using the partially observed markov process (POMP) framework. POMP models allow researchers to make inferences about the underlying dynamics of the system by linking observed and unobservable variables. Applications of these models have been carried out extensively in finance and epidemiology, but far less so in ecological systems.\n\n\nProposed Model\n\\[\\begin{equation}\n\\log F_{t+dt} = \\log F_t + dt(bR_t + c\\exp(\\log B_t)[1 - \\gamma R_t] - a)W_t^F\n\\end{equation}\\]\n\\[\\begin{equation}\n\\log B_{t+dt} = \\log B_t + dt(\\alpha + \\beta \\exp(\\log F_t)[1 - \\gamma R_t])W_t^F\n\\end{equation}\\]\nIn the first equation:\n\n\\(\\log F_{t+dt}\\) is the change in the log of the fox population density over a small time interval \\(dt\\);\n\\(\\log F_t\\) is the current log fox population density;\n\\(bR_t\\) represents the fox reproduction rate influenced by the rodent population density \\(R(t)\\);\n\\(a\\) represents the fox death rate;\n\\(c\\exp(\\log B_t)[1−\\gamma Rt]\\) represents the predation rate of foxes on birds, where \\(c\\) is a parameter representing the efficiency of fox predation, \\(\\exp(\\log B_t)\\) is the bird population density, \\(\\gamma\\) is a parameter representing the impact of rodent population on predation efficiency, and \\([1−\\gamma R_t]\\) accounts for the reduction in predation efficiency when the rodent population is high.\n\\(W_t^F\\) is an integrated gamma white noise variable;\n\\(t\\) is time.\n\nSimilarly, in the second equation:\n\n\\(\\log B_{t+dt}\\) is the change in the log of the ptarmigan population density over a small time interval \\(dt\\);\n\\(\\log B_t\\) is the current log ptarmigan population density;\n\\(\\alpha\\) and \\(\\beta\\) represent ptarmigan birth rate and the effect of fox predation on ptarmigan population, respectively;\n\\(\\exp \\log(F_t)\\) represents the fox population density influencing bird predation;\n\\(W_t^F\\) and \\(t\\) represent the same interpretations as in the first equation."
  },
  {
    "objectID": "projects/solar/index.html",
    "href": "projects/solar/index.html",
    "title": "Solar Flare Analysis",
    "section": "",
    "text": "Full report can be viewed here!\n\nSunspots are regions on the Sun’s surface with significantly stronger magnetic fields, appearing visibly darker due to lower temperatures compared to surrounding areas. These spots often come in pairs with opposite magnetic polarities and are composed of a darker umbra surrounded by a lighter penumbra. Monitoring and forecasting sunspots are crucial as they correlate strongly with solar phenomena like Coronal Mass Ejections (CMEs) and solar flares. These events can disrupt Earth’s magnetic fields, affecting satellite technology and communication systems such as GPS, highlighting the importance of sunspot activity in technological reliability and safety.\nSunspot activity also influences Earth’s climate, as evidenced by historical events like the Maunder Minimum, which coincided with the ‘Little Ice Age’ from 1645 to 1715. The relationship between sunspots and climate change is complex, showing minimal effects on short 11-year solar cycles but significant impacts over longer secular cycles of 100 to 200 years. Understanding these long-term trends is vital for predicting climate change and preparing for its consequences. Given the technological and climatic implications, robust research using various statistical models, including time series methods, is essential to capture and predict sunspot behavior and its broader impacts."
  },
  {
    "objectID": "projects/nhanes/index.html",
    "href": "projects/nhanes/index.html",
    "title": "VOC Analysis",
    "section": "",
    "text": "Authors: Pete Pritchard, Omar Afifi, Xiangru Pan\nWe investigate the presence of volatile organic compounds (VOCs) in human blood samples and their relationship with various levels of exposure to tobacco products. Our analysis demonstrates that, while not as exaggerated as might be expected, there is a significant relationship between VOC concentrations and exposure levels. By analyzing the results from our models, we also can draw conclusions about the difference in effect between second-hand and direct tobacco usage. We accomplish this objective by leveraging interpretable statistical learning models and thorough data analysis."
  },
  {
    "objectID": "projects/nhanes/index.html#introduction",
    "href": "projects/nhanes/index.html#introduction",
    "title": "VOC Analysis",
    "section": "Introduction",
    "text": "Introduction\nVolatile Organic Compounds (VOCs) are organic chemicals that humans are frequently exposed to. VOCs are commonly found in directly ingestible products like tobacco, as well as non-ingestible items like paints and detergents. Studying VOC levels in humans is important for a number of reasons including health monitoring, disease diagnosis, environmental exposure assessment, and biological research. Moreover, specific VOC patterns or signals may be associated with particular diseases or illnesses, which allows for quick, non-invasive diagnostic practices. This can be particularly valuable in cases where traditional diagnostic techniques are invasive, costly, or impractical. Consequently, understanding the impact of human VOC exposure holds great promise for the betterment of patient care, disease control, and biomedical research.\nThis analysis utilizes survey and blood sample data from the National Health and Nutritional Examination Survey (NHANES), provided by the Center for Disease Control (CDC). In particular, we analyze data pertaining to three primary VOCs commonly found in flammable tobacco products: Benzene, Ethyl-benzene, and Xylene, across various exposure levels (none, second-hand, direct). Using this analysis, we try to answer two fundamental questions: Do VOC concentrations have predictive power in classifying exposure levels? If so, which VOCs are more prevalent in smokers than in non-smokers, and how significant are the impacts of direct, as opposed to second-hand exposure?\nThe first part of this report briefly describes the data we used and contains a brief description of our data-processing methods. We subsequently describe three classification models that we employed (Support Vector Machines, Softmax Regression, and Decision Trees.) Each section describes our motivation for utilizing the respective model and discusses any relevant peculiarities or challenges associated with the respective method. Each section also provides results and conclusions that can be garnered from each model. Our final sections are directed towards summarizing our findings, and discussing reproducability and future research endeavors.\nOur analysis focuses on interpretable classifiers: While these do not provide the greatest accuracy, we are more interested in understanding the separability between smoker groups in terms of blood VOC levels rather than achieving prediction benchmarks, as well as determining which VOCs’ precedence is indicative of frequent tobacco use/exposure. Hence, we have avoided complex and uninterpretable models like deep neural networks or, to a lesser extent, boosted models."
  },
  {
    "objectID": "projects/nhanes/index.html#data",
    "href": "projects/nhanes/index.html#data",
    "title": "VOC Analysis",
    "section": "Data",
    "text": "Data\nWe utilize three datasets from NHANES: Cigarette Use [SMQ], Household Use [SMQFAM], and Volatile Organic Compounds Blood Tests [VOCWB]/[L04VOC]. We analyze a subset of the NHANES data that ranges from 1999-2018. Each participant’s blood test is linked to a survey response in the first two datasets, and these survey results form our response variable. In particular, participants were asked [1] if they currently regularly smoke, and [2] if they live with someone who regularly smokes. We grouped this into a single categorical response variable, ‘SMOKE’, which is interpreted according to the structure shown in Table 1.\n\n\n\nTable 1: Class Description.\n\n\n\n\n\n\n\n\n\nSMOKE\nDesciption\n\n\n\n\n0\nThe participant does not smoke and does not live with anyone who smokes\n\n\n1\nThe participant lives with someone who smokes, but does not smoke themselves\n\n\n2\nThe participant directly smokes tobacco on a regular basis\n\n\n\n\n\n\nThere is, of course, substantial overlap between classes 1 and 2: I.e., some individuals who smoke also live with a fellow smoker. Such participants are labeled as ‘2’. This allows us to isolate the effect of second-hand vs direct exposure. Moreover, Models like logistic regression and support vector machines rely on a fundamental assumption of linear separability (or at least separability in some tractable feature space). Unfortunately, this does not seem to be the case in the VOC data, but separability can be improved when logarithmic transformations are applied to the data, see Figure 1. This is due to the small values of the data: VOC concentrations are, in general, quite minuscule, so logarithmic transformations help to exaggerate the differences around the origin, yielding greater sensitivity to small fluctuations, which aids in classification accuracy.\nA subsequent issue is class imbalances. (Most people do not smoke). Moreover, most of the difficulty in this problem involves correctly identifying second-hand smoke exposure, and this makes up a minority class. Consequently, it is easy to achieve accuracy by simply ignoring the small classes, which leads to deceptively good results. Due to these distributional inequalities, we up-weighted the smaller classes in all of our models to achieve a balanced training objective. Despite hurting predictive power, this provides a more robust and truthful analysis. A visualization of the predictive space can be found in the Appendix Figure 5, and a frequency count of each class can be viewed in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Introducing a log transformation helps separate the data. (only non-smokers vs smokers is shown to aid in visualization) The right hand figure is the un-transformed data."
  },
  {
    "objectID": "projects/nhanes/index.html#methods-and-discussion",
    "href": "projects/nhanes/index.html#methods-and-discussion",
    "title": "VOC Analysis",
    "section": "Methods and Discussion",
    "text": "Methods and Discussion\n\nSupport Vector Machine\nOur motivation for using support vector machines in this problem space is largely due to the natural geometric implications of effective SVM-oriented prediction. The idea of separability (even in higher dimensional spaces), provides a nice interpretation because high classification accuracy suggests that VOC levels differ (i.e. are separable) between response levels. We selected hyper-parameters for our SVM models using 5-fold cross-validation. Test scores were calculated from held-out test data (i.e. not validation data).\n\n\n\nTable 2: SVM Classification Report.\n\n\n\n\n\nClass\nPrecision\nRecall\nF1\n\n\n\n\nNon Smokers\n.85\n.81\n.83\n\n\nSH Exposed\n.2\n.32\n.25\n\n\nSmokers\n.84\n.72\n.77\n\n\n\n\n\n\nIn conjunction with our log transformations, radial basis function (rbf) kernels provided superior performance over linear kernels. Our cross-validation-estimated optimal regularization parameter (\\(C\\)) and influence parameter (\\(\\gamma\\)) were both 1. The tuned models achieved an accuracy of .70, (although accuracies north of 75 percent can be obtained by ignoring the imbalances). Table 2 shows metrics across individual classes, and Table 3 shows both micro (weighted) and macro (unweighted) averages.\n\n\n\nTable 3: VM Averaged Outcomes Across all Response Levels.\n\n\n\n\n\n\nAccuracy\nPrecision\nRecall\nF1\n\n\n\n\nUnweighted Av.\n.7\n.63\n.62\n.62\n\n\nWeighted Av.\n.7\n.76\n.71\n.73\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: (Right) Confusion matrix from the Classifier. (Left) Barchart illustrating the class imbal- ances.\n\n\n\n\n\n\n\n\nUnsurprisingly, high classification accuracy is observed for smokers and non-smokers, but the classifiers do not perform well on the second hand-exposure group. Indeed, the recall score of .32 shows that the classifier is more or less unable to detect second-hand smokers, and the low precision score means that most of the positive predictions in this class were incorrect. The salient observation here is that the vast majority of mis-classification occurs when exposed individuals are mistaken by the model for smokers (204 vs. 149). This suggests that VOC levels in individuals exposed to tobacco through second-hand contact may more closely resemble that of smokers than of non-smokers. In our follow-up analysis, we built models that only classify smokers vs. exposed, non-smokers vs. exposed, and non-smokers vs smokers. These models had accuracy scores of \\(.70\\), .\\(78\\), .\\(91\\), respectively, reaffirming that exposed individuals are more separable from non-smokers than they are from smokers. (See Table 6 in the Appendix for a summary of these results.) This analysis suggests that the effects of second-hand tobacco exposure are not negligible, and may actually be fairly significant, although more thorough statistical analysis would be needed to make a definitive conclusion regarding this.\n\n\nSoftmax (Multi-class Logistic) Regression\nThe second method we employed was softmax regression. Much like SVM’s, it is not the most powerful predictive model, but it does provide valuable interpretability that allows us to obtain some insight into the effects of tobacco exposure. We dropped the log transformation to maintain interpretability. We also considered penalization terms, but we found that penalization terms were not helpful, as they encouraged the models to further ignore the second-hand group. We only discuss the outcome of the unpenalized model, but results from penalized softmax regression can be found in the Appendix Table 7. A performance summary of the un-penalized model is provided in Table 4:\n\n\n\nTable 4: Softmax Regression Classification Report (Without Penalty).\n\n\n\n\n\nClass\nPrecision\nRecall\nF1\n\n\n\n\nNon Smokers\n.73\n.91\n.81\n\n\nSH Exposed\n.32\n.25\n.28\n\n\nSmokers\n.72\n.58\n.64\n\n\n\n\n\n\nThe model performs well in identifying non-smokers, with a high precision of 0.73 and recall of 0.91, indicating that it correctly identifies the majority of non-smokers while minimizing the number of false positives. However, its performance in predicting second-hand smokers is weaker, with a lower precision of 0.32 and recall of 0.25, indicating that it fails to capture a significant portion of actual second-hand smokers and often misclassifies other classes as second-hand smokers. For smokers, the model demonstrates moderate performance, with a precision of 0.72 and recall of 0.58, suggesting that it correctly identifies a large portion of smokers but may also misclassify some non-smokers or second-hand smokers as smokers.\nOur main motivation for using softmax regression was to obtain the coefficients, which provide valuable insight into which VOCs are more indicative of frequent tobacco use. The coefficients of the model are provided below in Table Table 5. Each row corresponds to a response level, and columns correspond to predictors. The coefficients represent the change in the log-odds of being in each smoking status category for a one-unit increase in the corresponding predictor variable, holding all other variables constant. For non-smokers, the odds of being classified as such decrease significantly with the first predictor variable (Benzene) \\(e^{-11.8812258} \\approx 0.000012\\) meaning that, for each additional nanogram of Benzene observed in a blood sample, the odds of being a non-smoker decrease by almost 100 percent. Additionally, higher values of the Ethyl-benzene decrease the odds of being a non-smoker by approximately 28.2 percent: \\(e^{-0.33048418} \\approx  0.718\\), so that the impact Ethyl-benzene is not as significant as the impact of Benzene. Conversely, the presence of an additional nanogram of Xylene actually slightly increases the odds of being a non-smoker by about 24.9 percent. However, this negative relationship may be due to colinearity with other predictors. (For example, the direction of the relationship may change if we remove Benzene from the model, or the coefficient of Ethyl-Benzene may increase). Similar interpretations can be made for predicting second-hand smokers and smokers, with higher Benzene and Ethyl-benzene levels being associated with increased odds of being in the exposed/smoking classes, and higher Xylene levels having the opposite interpretation.\nThe confusion matrix Figure 3 shows us how the model performs across each class. Similar to the SVM, it does well in classifying smokers and non-smokers but performs more poorly in trying to classify the second-hand group. One interesting difference is that, whereas the SVM mostly miss-classified this group as smokers, this model more commonly makes the opposite error. This calls into question how severe the effects of second-hand exposure are. We believe this difference is due to the use of r.b.f. kernels in the SVM’s: with more dimensionality, the differences between second-hand smokers and non-smokers may become more pronounced, but these differences are too minute to be picked up by linear decision boundaries.\n\n\n\n\n\n\nFigure 3: Confusion Matrix for Softmax Regression.\n\n\n\n\n\n\nTable 5: Softmax Regression Coefficients.\n\n\n\n\n\nClass\nBenzene\nEthylbenzene\nXylene\n\n\n\n\nNon Smokers\n-11.8812258\n-0.33048418\n0.22244872\n\n\nSH Exposed\n3.6394399\n-0.46673514\n0.17573673\n\n\nSmokers\n8.2417859\n0.79721931\n-0.39818545\n\n\n\n\n\n\n\n\nClassification Trees\nFor our final model, we elect to implement a decision tree classifier. Decision trees are popular for a wide variety of reasons, but our primary motive is interpretability and the ability to determine which features have the most importance. We are not just concerned with predicting a class for a particular terminal node region; we also want to understand the distribution of classes among the training data points that fall into that region. Similar to the SVM process, we set our decision tree’s hyperparameters by 5-fold validation, and error metrics were calculated using a seperate test set.\n\n\n\n\nAccuracy\nPrecision\nRecall\nF1\n\n\n\n\nUnweighted Av.\n.71\n.61\n.59\n.56\n\n\nWeighted Av.\n.71\n.87\n.71\n.77\n\n\n\nAkin to our results from previous models, prediction accuracy for those in the exposed class is much weaker than those in the smoker and non-smoker classes. Figure 4 depicts a feature importance plot, a visualization that provides an understanding of how significant each feature is to the model’s decision-making process. The metric used to calculate how much each feature contributes to our model’s prediction accuracy is based on Gini impurity. We can see that Xylene is clearly favored by our tree’s decision process. We assume this is consistent with the discovery of the negative relationship between Xylene and non-smokers that was found in our multinomial regression model. Often, decision trees are biased towards features that take on a large range of values relative to other predictors, but this is not made immediately obvious by what is shown by the histograms in Figure 5. Upon further examination, we speculate that Xylene’s influence stems from having a larger proportion of observation greater than 0.4 ng/mL compared to Ethyl-benzene and Benzene, however, further statistical analysis is required to confirm this hypothesis.\n\n\n\n\n\n\n\n\nFigure 4: Feature Importance plot and Confusion Matrix for Decision Tree Classifie"
  },
  {
    "objectID": "projects/nhanes/index.html#concluding-remarks-reproducibility",
    "href": "projects/nhanes/index.html#concluding-remarks-reproducibility",
    "title": "VOC Analysis",
    "section": "Concluding Remarks & Reproducibility",
    "text": "Concluding Remarks & Reproducibility\nOur analysis shows that there is a clear (mostly positive) relationship between increased VOC levels and tobacco exposure. This is demonstrated by achieving reproducible statistical models that achieve predictive accuracy using VOC concentrations as predictors. While the differences in effect between non-exposure and direct exposure is stark, there are less clear and even contradictory conclusions that can be drawn regarding the effects of second-hand exposure. In particular, SVM predictions suggest that the distinction between second-hand and non-exposure is minimal, whereas Decision Tree and Softmax Regression Methods suggest that individuals with second-hand exposure more closely resemble their smoking counterparts. This may be due to inconsistent data collection: One potential caveat in this analysis is our assumption that individuals who live with smokers are exposed to tobacco regularly. This may not be entirely accurate, and follow-up work may want to investigate the relationship between second-hand and direct exposure more closely by isolating for the nature of the exposure (for example, distinguishing between participants who live with a smoker who smokes indoors, versus those who live with a more considerate smoker.) Another possibility for follow-up work is to examine whether or not the usage of r.b.f. kernels in SVM classifiers aids in this behavior, and attempt to identify which aspects of data contribute to this phenomena.\nAll of our code can be found on github."
  },
  {
    "objectID": "projects/nhanes/index.html#appendix",
    "href": "projects/nhanes/index.html#appendix",
    "title": "VOC Analysis",
    "section": "Appendix",
    "text": "Appendix\n\nThe Distribution of VOC’s Across Different Response Levels\n\n\n\n\n\n\nFigure 5: Distributions of the three VOC’s within different response levels: A square root transformation is applied to aid with visualization.\n\n\n\n\n\nOne-vs-One SVM Results\n\n\n\nTable 6: Accuracy for one-vs-one SVM Classifiers. Note: In this case, balances are imposed, so percision, recall, and F1 reduce to just accuracy scores.\n\n\n\n\n\n\nAccuracy (With Imbalance Correction)\n\n\n\n\nSmoker vs. Non-Smoker\n.91\n\n\nNon-Smoker vs. Exposed\n.78\n\n\nExposed vs. Smoker\n.70\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Confusion Matrices for one-vs-one SVM Classifiers.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficients and Confusion Matrices for Penalized Softmax Regression\nThis Section contains some figures and tables showing the outcome of penalizing our softmax model from section 3.2. Note the stark drop in performance for the second-hand group (in particular the low precision and recall scores).\n\n\n\nTable 7: Multinomial Regression Classification Report (With Penalty).\n\n\n\n\n\nClass\nPrecision\nRecall\nF1\n\n\n\n\nNon Smokers\n.69\n.94\n.81\n\n\nSH Exposed\n.27\n.01\n.01\n\n\nSmokers\n.68\n.78\n.73\n\n\n\n\n\n\n\n\n\nTable 8: Multinomial Regression Coefficients.\n\n\n\n\n\nClass\nBenzene\nEthylbenzene\nXylene\n\n\n\n\nNon Smokers\n-15.59127689\n0.47948689\n-0.0909995\n\n\nSH Exposed\n5.32591981\n-2.03059051\n0.7141749\n\n\nSmokers\n10.26535708\n1.55110361\n-0.6231754\n\n\n\n\n\n\n\n\n\nConfusion Matrix for Penalized Softmax Regression."
  },
  {
    "objectID": "projects/monteCarlo/index.html",
    "href": "projects/monteCarlo/index.html",
    "title": "BSM Monte Carlo",
    "section": "",
    "text": "Predicting the direction of stock prices is an enticing provocation with the promise of a life of luxury and philanthropy. Alas, such a challenge is generally considered impossible given the complex nature of the stock market. However, there exists mathematical and statistical tools that can help us make useful predictions, manage risk, and profit probabilistically. This report serves as a both a introductory glimpse of the mathematical underpinnings as well as a reference to a number of theoretical and pragmatic resources on options trading. We briefly review the Black-Scholes European option pricing model and then solve it using the Monte-Carlo stochastic modeling technique.\n\nThe Black-Scholes model is a mathematical formula used to estimate the fair price or theoretical value for a European call or put option. The model was developed by Fischer Black and Myron Scholes in 1973 in their paper The pricing of options and corporate liabilities, and later extended by Robert Merton (Black and Scholes 1973). It is widely used in finance for option pricing, and has been recognized as a significant contribution to the development of modern financial mathematics theory. Scholes and Merton would eventually earn the 1997 Nobel Prize in Economics for their contributions. Black was ineligible for the award as he died a few years prior, however he was listed as a contributor.\nThe Black-Scholes model provides a way to determine the fair value of an option, which can be compared to the actual market price to determine if the option is overpriced or underpriced. It uses partial differential equations to calculate the expected payoff of the option at expiration, discounted to present value using the risk-free interest rate. The model has certain limitations, such as assuming constant volatility and not accounting for factors such as dividends or early exercise, but it is still widely used in practice and has led to the development of other models that incorporate additional factors. One of these popular model developments is incorporating Monte Carlo simulations.\nMonte Carlo methods are a broad class of computational modeling techniques used to simulate and analyze complex systems or processes. This method and its name were inspired by the casinos of Monaco, and its approach involves generating a large number of random samples or “trials” to make conclusions with statistical significance. This technique has become wildly popular in computational finance, and is particularly useful when an option pricing formula is difficult or impossible to derive analytically (Johnson 2010).\nThe remainder of this report is structured as the following; sections 2 and 3 take a deeper look into the mathematical underpinnings of the Black-Scholes and Monte Carlo methods for European option pricing. Section 4 demonstrates how to computationally put the models into production using Python, and compares the results of the two methods using simulated financial data. Lastly, we conclude and and mention a few ways this report can be extended.\n\nPreliminaries\n\nA call option is a financial contract between a buyer and a seller that gives the buyer the right, but not the obligation, to buy an underlying asset at a specified price, known as the , at, or before, a certain date called \\(maturity \\ T\\) or the . The underlying asset can be a stock, currency, commodity, or any other financial instrument. The buyer purchases the option from the seller at price \\(C(t)\\) at time \\(t&lt;T\\) (Asiri 2018).\nA put option is a financial contract between a buyer and a seller that gives the buyer the right, but not the obligation, to sell an underlying asset at a specified price, known as the , at, or before, a certain date called \\(maturity \\ T\\) or the . The underlying asset can be a stock, currency, commodity, or any other financial instrument. The buyer purchases the option from the seller at price \\(P(t)\\) at time \\(t&lt;T\\) (Asiri 2018).\n\nThe buyer of the call/put option pays a premium to the seller for the right to buy/sell the underlying asset. For the call option, if the price of the underlying asset rises above the strike price \\(K\\) before the expiration date, the buyer can exercise the option and buy the asset at the lower strike price, and then sell it on the open market for a profit. Whereas, for the put option, if the price of the underlying falls below the strike price before the expiration date, the buyer can exercise the option and sell the asset at a higher strike price, and then sell it on the open market for a profit.\nA call or put option of an underlying option has a market, or today, price \\(S(t)\\) and a future price \\(S(T)\\). We say a call option expires in the money when \\(S(T)&gt;K\\), at the money when \\(S(T)=K\\), and when \\(S(T)&lt;K\\). When an option expires out of the money, the buyer simply chooses not to exercise the option, and it expires worthless. For put options the lingo is similar, however the buyer is anticipating the asset price to drop, thus in the money when \\(S(T)&lt;k\\), so on and so forth.\n\n\nFormula\n\\[\\begin{equation}\nC(S_{0},t)=S_{0}N(d_{1})-Ke^{-r(T-t)}N(d_{2})\n\\end{equation}\\] \\[\\begin{equation}\nP(S_{0},t)=N(-d_{2})Ke^{-r(T)}-N(d_{1})S_{0},\n\\end{equation}\\]\nwhere\n\n\\(S_{0}\\) is the stock price;\n\\(C(S_{0},t)\\) is the price of a call option as a formulation of the stock price and time;\n\\(P(S_{0},t)\\) is the price of a put option as a formulation of the stock price and time;\n\\(K\\) is the strike price;\n\\((T-t)\\) is the time to maturity, i.e. the exercise date \\(T\\), less the amount of time between now \\(t\\) and then. Generally this is represented in years with one month equaling \\(1/12\\);\n\\(N(d_{1})\\) and \\(N(d_{2})\\) are the cumulative distribution functions for a standard normal distribution with the following formulation: \\[\\begin{equation}\n  d_{1}=\\frac{ln\\frac{S_{0}}{K}+(r+\\frac{\\sigma ^{2}}{2})(T-t)}{\\sigma\\sqrt{T-t}}\n\\end{equation}\\] \\[\\begin{equation}\n  d_{2}=d_{1}-\\sigma\\sqrt{(T-t)},\n\\end{equation}\\]\n\nwhere\n\n\\(\\sigma\\) represents the underlying volatility;\n\\(r\\) is the risk-free interest rate \n\n\n\nLimitations\nThe Black-Scholes model makes a number of assumptions, however one of the most glaring issues is that it assumes stock returns are normally distributed. This implies that the volatility of the market is constant over time. Figure 1 shows the rolling average standard deviation for Microsoft daily close price over approximately ten years. Notice that the volatility is in no way stable.\n\n\n\nRolling Volatility\n\n\nAnother key limitation of the Black-Scholes model is that it underestimates the tail density. Figure 2 shows a kernel density estimation plot with the empirical versus normal distribution for the Microsoft stock. Notice there are more returns at the extremities of the distributions than the normal distribution would predict. The phenomenon is known as “fat tails” or “excessive kurtosis,” and this suggests that the model will underestimate the value of out-of-the-money options (John 2020a).\n\n\n\nTail Density\n\n\n\n\nMonte Carlo Simulation\nMonte Carlo simulations are a popular and widely used method for pricing European options. This approach is particularly useful when the option pricing formula is difficult or impossible to derive analytically. In the Monte Carlo method, the future price of the underlying asset is modeled using random simulations, and the option price is calculated as the discounted expected value of the option’s payoff at the option expiration time.\nEconomist Phelim Boyle is credited for initiating the use of Monte Carlo methods in option pricing in his paper Options: A Monte Carlo Appraoch. One advantage of Boyle’s Monte Carlo approach is that it can be easily extended to handle more complex option types, such as options with early exercise features or options with multiple underlying assets. Additionally, the Monte Carlo method can be used to estimate the sensitivities of option prices to changes in the underlying asset’s parameters, such as volatility or interest rates (Boyle 1977).\nHowever, the Monte Carlo method can be computationally expensive, as it requires the generation of a large number of price paths to obtain an accurate estimate of the option price. Various techniques have been developed to improve the efficiency of Monte Carlo simulations, including variance reduction techniques such as control variates, antithetic variates, and importance sampling. Despite its computational complexity, the Monte Carlo simulation remains a valuable and widely used method for pricing European options.\n\n\nMore Preliminaries\n\nWiener Process, also known as Brownian motion, is a continuous stochastic process \\(W(t)\\) with the following properties:\n\n\\(W(0)=0\\).\nFor each \\(t\\), the random variable \\(W(t)\\) is normally distributed with mean 0 and variance \\(t\\).\nFor each \\(t_{1}&lt;t_{2}\\), the normal random variable \\(W(t_{2})-W(t_{1})\\) is independent of the random variable \\(W(t_{1})\\), and independent of all \\(W(s),0\\le s\\le t_{1}\\).\nWiener Process \\(W(t)\\) can be represented by continuous paths (Sauer 2006).\n\n\nThere are several research articles that take a deeper look into geometric Brownian motion, however, for the purpose of this topic, I found this interactive blog to be very clear and well explained . Figure 3 visualizes simulated assets paths using Geometric Brownian motion.\n\nIto’s Lemma performs as the chain rule for stochastic calculus, similar to the chain rule in ordinary differential calculus. Specifically, it relates the partial differential of a function of a stochastic process to the drift rate, volatility, and their partial derivatives. The Ito formula is given below:\n\nIf \\(y=f(t,x)\\), then\n\\[\\begin{equation}\n    dy=\\frac{\\partial f}{\\partial x}(t,x)dt+\\frac{\\partial f}{\\partial x}(t,x)dx+\\frac{1}{2}\\frac{\\partial ^{2}f}{\\partial x^{2}}(t,x)dxdx,\n\\end{equation}\\]\nwhere the \\(dx\\)\\(dx\\) term is interpreted by using the identities \\(dt\\)\\(dt=0\\), \\(dt\\)\\(dB_{t}=dB_{t}\\)\\(dt=0\\), and \\(dB_{t}dB_{t}=dt\\) (Sauer 2006; Goodman 2018).\nTo apply the Monte Carlo simulation to price a European option, the first step is to generate a large number of random price paths for the underlying asset using a stochastic process such as geometric Brownian motion. Then, the payoff of the option is calculated for each of these price paths, and the average value is computed. Finally, the option price is determined by discounting the average payoff value to the present time.\nThe following equation was solved by the author of this website (John 2020b).\nAssume the stock price \\(S\\), pays annual dividend \\(q\\) and has expected return \\(\\mu\\) equal to the risk free rate \\(r-q\\), the volatility \\(\\sigma\\) is assumed to be constant.\nEssentially, the stock price can be modeled by a partial differential equation in which at least one of the terms is a stochastic random process. Let’s consider the case when volatility is 0.\n\\[\\begin{equation}\n    dS=\\mu Sdt\n\\end{equation}\\] Given the price of the stock now \\(S_{0}\\), we can calculate the price \\(S_{T}\\) at given time \\(T\\) by separating and integrating as follows: \\[\\begin{equation}\n    \\int_{0}^{T}\\frac{dS}{S}=\\int_{0}^{T}\\mu dt\n\\end{equation}\\] Which gives: \\[\\begin{equation}\n    S_{T}=S_{0}e^{\\mu T}\n\\end{equation}\\] \\[\\begin{equation}\n    ln(S_{T})=ln(S_{0})+\\int_{0}^{T}\\mu dt\n\\end{equation}\\] We need to include a stochastic term in the equation above to account for randomness in stock prices. By doing so, we get the following: \\[\\begin{equation}\n    dS=S\\mu dt+S\\sigma dW(t)\n\\end{equation}\\] Where \\(W_{t}\\) is a Wiener Process, and now he equation is in the form on an Ito process. Ito’s lemma states if a random variable follows an Ito process then another twice differentiable function \\(G\\) described by the stock price \\(S\\) and time \\(t\\) also follows an Ito process: \\[\\begin{equation}\n    dG=(\\frac{\\partial G}{\\partial S}S\\mu +\\frac{\\partial G}{\\partial t}+\\frac{1}{2}\\frac{\\partial ^{2}{G}}{\\partial S^{2}}S^{2}\\sigma ^{2})dt+\\frac{\\partial G}{\\partial S}S\\sigma dW(t)\n\\end{equation}\\] Applying Ito’s lemma to \\(ln(S)\\) first we calculate the partial derivatives with respect to \\(t\\) and \\(S\\) as follows: \\[\\begin{equation}\n    G=ln(s)\n\\end{equation}\\] \\[\\begin{equation}\n    \\frac{\\partial G}{\\partial S}=\\frac{1}{S},\\ \\frac{\\partial G}{\\partial t}=0,\\ \\frac{\\partial ^{2} G}{\\partial S^{2}}=-\\frac{1}{S^{2}}\n\\end{equation}\\] Plugging the partial derivatives into Ito’s lemma gives: \\[\\begin{equation}\n    dG=(\\frac{1}{S}S\\mu + 0-\\frac{1}{2}\\frac{1}{S^{2}}S^{2}\\sigma ^{2})dt+\\frac{1}{S}S\\sigma dW(t)\n\\end{equation}\\] \\[\\begin{equation}\n    =(\\mu - \\frac{\\sigma ^{2}}{2})dt+\\sigma dW(t)\n\\end{equation}\\] Therefore, the distribution of \\(ln(S_{T})-ln(S_{0})=(\\mu -\\frac{\\sigma ^{2}}{2}T+\\sigma \\sqrt{T}\\). The distribution of the stock price at expiration is given by rearranging the previous equation and taking the exponential of both sides: \\[\\begin{equation}\n    S_{T}=S_{0}e^{(\\mu -\\frac{\\sigma ^{2}}{2})dt+\\sigma dW(t)}\n\\end{equation}\\] This can also be written as: \\[\\begin{equation}\n    ln(S_{t})=ln(S_{0})+\\int_{0}^{t}(\\mu -\\frac{\\sigma ^{2}}{2})dt+\\int_{0}^{t}\\sigma dW(t), \\ \\ for \\ t\\in [0,\\cdots ,T]\n\\end{equation}\\]\n\n\n\nGeometric Brownian Motion\n\n\n\n\nComputation\nThe following code was written using the Python programming language (version 3.8.2) along with the pandas, numpy, scipy, and matplotlib libraries. We use equation (1) and equation (17) to computer the estimate of a one month call option, or \\(T=\\frac{1}{2}\\) years, with the following parameter values: \\(S=100\\), \\(K=110\\), \\(r=0.05\\), \\(q=0.02\\), and \\(\\sigma =0.25\\). A further look into the implementation of this code, and more work by the author can be found here (John 2020b).\ndef black_scholes_call(S,K,T,r,q,sigma):\n    \"\"\"\n    Inputs\n    # S = Current stock Price\n    # K = Strike Price\n    # T = Time to maturity 1 year = 1, 1 months = 1/12\n    # r = risk free interest rate\n    # q = dividend yield\n    # sigma = volatility \n    \n    Output\n    # call_price = value of the option \n    \"\"\"\n    d1 = (np.log(S/K) + (r - q + sigma**2/2)*T) / sigma*np.sqrt(T)\n    d2 = d1 - sigma* np.sqrt(T)\n    \n    call = S * np.exp(-q*T)* norm.cdf(d1) - K * np.exp(-r*T)*norm.cdf(d2)\n    return call\ndef geo_paths(S, T, r, q, sigma, steps, N):\n    \"\"\"\n    Input Parameters\n    # S = Current Stock Price\n    # K = Strike Price\n    # T = Time to maturity \n    # r = sirk free interest rate\n    # q = dividend yield\n    # sgima = volatility\n    \n    Output\n    # [steps, N] Matrix of asset paths\n    \"\"\"\n    dt = T/steps\n    # S_{T} = ln(S_{0})+\\int_{0}^T(\\mu-\\frac{\\sigma^2}{2})dt+\\int_{0}^T \\sigma dW(t)\n    ST = np.log(S) +  np.cumsum(((r - q - sigma**2/2)*dt +\\\n                              sigma*np.sqrt(dt) * \\\n                              np.random.normal(size=(steps,N))),axis=0)\n    \n    return np.exp(ST)\n\n\n\n\n\n\n\n\n\n\nMCE(100)\nMCE(1000)\nMCE(10000)\nMCE(100000)\nBlack-Scholes\n\n\n\n\nC(100) = 4.0461\nC(100) = 4.0003\nC(100) = 3.8979\nC(100) = 3.8187\nC(100) = 3.7451\n\n\n\nUsing the parameters given in Section 5, we will compare the accuracy of the Monte Carlo Methods to the exact Black-Scholes Equation. We repeat the procedure, increasing the number of trails \\(N\\) from 100 to 100,000, and monitor how accuracy increases or decreases. The results are given in the table above. Notice how as \\(N\\) increases the price approaches the Black-Scholes price.\n\n\nConclusion\nAs previously mentioned, this report is just the tip of the options pricing iceberg. There is seemingly no limit to the number of directions once can build upon Black, Scholes, and Merton’s historical work. The exploding development in technology over the past few decades has led to intense research and interest in this field. Mathematicians, Physicists, and Computer Scientists are flocking to Wall Street to build market-beating strategies. Although the Black-Scholes equation is a powerful, flexible, and easy to use tool for option pricing, it has its limitations. Financial traders and investors should be conscious of these drawbacks to use the equation appropriately.\nMonte Carlo methods are more computationally expensive, take more time to calculate, and are less accurate for the simplest situations, however a closed solution like Black-Scholes is not always readily available or applicable. Furthermore, Monte Carlo provides a way to extended the Black-Scholes equation to a wider range of option types, and opens the doors to apply methods to more dynamic cases. Regardless of the method, both Black-Scholes and Monte Carlo simulation have helped to legitimize options trading, and serve as the fundamental strategy of eliminating risk associated with volatility, known as hedging.\nA few simple ways in which this study can be extended:\n\nDevelop a Monte Carlo method for American options or options with multiple underlying assets\nImprove the efficiency of the Monte Carlo Method using techniques such as control variates, antithetic variates, and importance sampling\nTest accuracy using real world financial data against actual brokers.\n\n\n\n\n\n\nReferences\n\nAsiri, Fatimah Fathalden. 2018. “The Price of Stocks, Geometric Brownian Motion, and Black Scholes Formula.” Major Papers 26.\n\n\nBlack, Fischer, and Myron Scholes. 1973. “The Pricing of Options and Corporate Liabilities.” The Journal of Political Economy 81 (3): 637–54.\n\n\nBoyle, P. 1977. “Options: A Monte Carlo Approach.” Journal of Financial Economics 4 (3): 323–38.\n\n\nGoodman, Jonathon. 2018. “Lesson 4, Ito’s Lemma.” Courant Insitute NYU.\n\n\nJohn. 2020a. “Black Scholes Model Python.” 2020. https://www.codearmo.com/python-tutorial/options-trading-black-scholes-model.\n\n\n———. 2020b. “Pricing Options by Monte Carlo Simulation with Python.” 2020. https://www.codearmo.com/blog/pricing-options-monte-carlo-simulation-python.\n\n\nJohnson, A. M. 2010. “Monte Carlo Methods.” In Encyclopedia of International Education (Third Edition), 296–303. Elsevier.\n\n\nSauer, Tim. 2006. Numerical Analysis. Reading, MA: Pearson Addison-Wesley."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pete Pritchard",
    "section": "",
    "text": "Bio\nI’m a second year Masters of Applied Statistics student at the University of Michigan, Ann Arbor. I obtained Bachelor’s degrees in Economics and Applied Mathematics at the University of Tennessee, Chattanooga. And, before that, I spent four years living in Northern Spain, studying Marketing part-time at the University of Navarra (but mainly traveling and learning more about different cultures and myself).\nCurrently (and for the second summer in a row), I am engineering advanced analytics platforms for internal usage at NASA’s Glenn, Ames, and Langley Research Centers. I primarily use Python to build real-time data applications of operational and test data to provide comprehensive visualizations, reveal patterns and drive more effective decision-making and business outcomes for a portfolio of wind tunnel facilities. Learn more about Aerosciences Evaluation and Test Capabilities Portfolio Office (AETC) here!\n\n\nResearch\nMy research experience is rather broad. While an undergraduate student, I was fortunate enough to work under three different professors in various fields. First, I applied econometric methods to explore the dynamic connectedness of various currencies. Next, I assisted in the development of a mixed-integer linear model aimed to optimally locate emergency service vehicles. And lastly, I built deep neural network models to forecast the spread of infection diseases using trending Google Search key words.\n\n\nTeaching\nWhile completing my studies at The University of Michigan, I have had the pleasure of working as a Graduate Student Instructor for the Introduction to Statistics and Data Analysis undergraduate course (STATS 250). I’ve thoroughly enjoyed this aspect of my time as a graduate student, and would love to incorporate teaching into my work life from here on out. Some of the most interesting and influential people in my life are highschool teachers, and I would love to one day fill those shoes.\n\n\nPersonal Life\nI’ve spent my formative years in Tennessee, but have called Mexico and Spain home at various points in my life. Since 2023, I have lived in Ann Arbor, MI, but anticipate that I will move to a large, coastal city once I graduate from UofM.\nIn my freetime, I enjoy watching films, cooking, going to the gym, reading, and watching my favorite sports teams (Tennessee Titans and Atlanta Braves). I developed an obsession with movies around my 19th birthday, and since then I’ve watched an almost-alarming number of films, documentaries and series. If you are interested in a recommendation or would like to share your favorites, please do not hesitate to reach out by email!"
  }
]